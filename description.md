Це комплексний застосунок для **геопросторового аналізу**, який поєднує класичне комп'ютерне бачення (Object Detection) та сучасні великі мультимодальні моделі (VLM). Його мета — перетворити «сирі» пікселі супутникових знімків на структуровану базу даних об'єктів (будівлі, транспорт), яку можна запитувати людською мовою або використовувати для пошуку змін на місцевості.

Нижче наведено детальний опис роботи системи, розбитий на логічні етапи пайплайну.

### Етап 1: Підготовка та нарізка (Tiling)

Процес починається з отримання великого супутникового файлу, зазвичай у форматі GeoTIFF. Оскільки такі зображення можуть мати розмір у десятки тисяч пікселів, їх неможливо подати в нейромережу цілком. Тому на першому етапі вмикається модуль нарізки. Він приймає шлях до файлу і параметри сітки. Модуль віртуально накладає на карту сітку з квадратів (тайлів), наприклад, 1024x1024 пікселі, з певним перекриттям (overlap), щоб об'єкти на краях не були розрізані навпіл.

На виході цього етапу формується потік об'єктів-посилань (TileRef). Кожен такий об'єкт містить не саме зображення (щоб не забивати пам'ять), а інструкцію: «цей шматок знаходиться в таких-то координатах пікселів і відповідає таким-то географічним координатам (широта/довгота)». Лише коли черга доходить до обробки, система зчитує конкретний квадрат пікселів у пам'ять у форматі RGB-масиву (numpy array).

### Етап 2: Фільтрація (Gating)

Отриманий RGB-тайл потрапляє до модуля фільтрації, який називається Gate. Його задача — заощадити гроші та час, відкинувши "нудні" зображення, де немає нічого важливого (ліс, вода, суцільне поле). Цей модуль приймає масив пікселів і повертає булеве рішення: "залишити" або "викинути".

У коді реалізовано три підходи до цього рішення. 
###### Евристичний фільтр (Heuristic Gate)

Це "математичний" підхід. Він вимірює статистичні властивості зображення, базуючись на припущенні, що антропогенні об'єкти (будівлі, дороги) виглядають складніше і контрастніше, ніж природа (поля, вода). Модуль переводить кольорове зображення у відтінки сірого і розраховує два показники. Перший — це щільність країв (Edge Density). Використовується алгоритм Кенні (Canny edge detector), який шукає різкі переходи яскравості. Якщо на зображенні багато ліній, кутів і меж (що характерно для забудови), цей показник буде високим.

Другий показник — це ентропія (Entropy). Вона вимірює "хаотичність" розподілу пікселів. Однотонне поле або вода мають низьку ентропію, тоді як міська забудова з різними кольорами дахів та асфальтом має високу ентропію. Фінальне рішення приймається за формулою: Score = 0.6 * (нормалізовані краї) + 0.4 * (нормалізована ентропія). Якщо цей бал вищий за поріг (за замовчуванням 0.24), тайл проходить далі.

Це найшвидший метод, він працює миттєво навіть на слабкому процесорі. Він не потребує відеокарти (GPU) і не завантажує важких моделей у пам'ять. Його легко налаштувати "на льоту", просто змінивши порогові числа в конфігурації. Однак він "дурний" у розумінні контексту. Густий ліс у сонячний день може мати високу ентропію та багато країв (тіні від дерев), через що фільтр помилково пропустить його як "цікавий". І навпаки, великий гладкий бетонний ангар може мати низьку щільність країв і бути відсіяним.
###### Семантичний фільтр (CLIP Gate)

Це "інтелектуальний" підхід, що базується на розумінні змісту. Він використовує модель CLIP (Contrastive Language-Image Pre-training), яка вміє порівнювати зображення з текстом. При ініціалізації система завантажує модель CLIP і кодує два набори текстових фраз у вектори. Перший набір — "Keep Prompts" (що ми шукаємо: "міська зона", "будівлі", "парковка"). Другий набір — "Drop Prompts" (що ігноруємо: "ліс", "поле", "хмари"). Коли надходить тайл, CLIP перетворює його зображення на числовий вектор. Далі математично обчислюється відстань (схожість) між вектором картинки та векторами текстових фраз.Система дивиться, до якої групи описів картинка ближча. Розраховується різниця: Score = (схожість з Keep) - (схожість з Drop). Якщо різниця позитивна і перевищує поріг (наприклад, 0.15), тайл вважається цікавим. 

Фільтр розуміє контекст. Він може відрізнити "парковку" від "ріллі", навіть якщо візуально вони схожі за текстурою, бо він "бачив" такі об'єкти під час навчання. Він дуже гнучкий: щоб почати шукати щось інше (наприклад, тільки водойми), не треба перенавчати модель — достатньо змінити текстові промпти в налаштуваннях. Однак це найповільніший метод. CLIP — важка нейромережа, яка потребує багато пам'яті та, бажано, GPU. Обробка кожного тайлу займає відчутний час, що може сповільнити весь пайплайн у 10-20 разів порівняно з евристикою.
###### Нейромережевий класифікатор (CNN Gate)

Це "спеціалізований" підхід. Він використовує легку класичну згорткову нейромережу ( MobileNetV3), яка була спеціально навчена на попередньо розмічених даних розрізняти "цікаві" та "нудні" тайли. Модуль бере тайл, зменшує його до розміру 224x224 пікселі (стандарт для таких мереж) і нормалізує кольори. Зображення проходить через шари нейромережі, яка виділяє візуальні ознаки (форми, текстури). На виході мережа має один нейрон із функцією активації Sigmoid, який видає число від 0 до 1. Це число трактується як ймовірність того, що тайл є "цікавим". Якщо ймовірність вища за поріг (наприклад, 0.5), тайл пропускається.

Це золота середина. Він набагато розумніший за евристику (бо бачить форми об'єктів), але набагато швидший за CLIP (MobileNet дуже легка і оптимізована модель). Якщо є хороший датасет для навчання, цей метод дає найкращу точність при високій швидкості. В той же час це "чорна скринька", яка потребує навчання. Щоб цей фільтр запрацював, потрібно спочатку зібрати тисячі прикладів тайлів "ліс" і "місто", запустити скрипт навчання (train_cnn_gate.py) і отримати файл ваг моделі (.pt). Не можна просто змінити налаштування тексту, як у CLIP; якщо зміниться тип місцевості (наприклад, пустеля замість лісу), модель доведеться перенавчати заново.

### Етап 3: Детекція об'єктів (Object Detection)

Тайли, що пройшли фільтр, передаються детектору. Цей модуль використовує модель YOLO (You Only Look Once) — швидку нейромережу для пошуку об'єктів. На вхід подається RGB-зображення тайлу. Детектор "дивиться" на нього і шукає відомі йому класи: машини, вантажівки, будівлі тощо.

Виходом цього етапу є список "bounding boxes" — прямокутних рамок. Кожна рамка містить координати кутів у межах цього маленького тайлу (локальні пікселі), назву знайденого класу (наприклад, "car") та рівень впевненості моделі (score). Також на цьому етапі підраховується статистика класів для тайлу, наприклад: "тут знайдено 5 машин і 1 будівлю". Якщо детектор нічого не знайшов, тайл може бути відсіяний, щоб не навантажувати наступний, найдорожчий етап.

### Етап 4: Семантична анотація (VLM Annotation)

Це "інтелектуальне ядро" системи. Якщо на тайлі знайдені об'єкти, система звертається до VLM (Vision-Language Model). Перед цим спрацьовує кеш: система перевіряє, чи не аналізувала вона цей тайл із таким самим набором знайдених рамок раніше. Якщо ні — формується запит.

На вхід VLM подається саме зображення тайлу та текстовий список знайдених детектором рамок. Промпт (інструкція) просить модель подивитися на ці конкретні місця і описати їх детально: якого кольору дах у будівлі, з якого вона матеріалу, який тип транспорту, що відбувається на сцені загалом. VLM повертає суворий JSON-об'єкт, де кожній рамці додано атрибути (roof_color, material, construction_state). Таким чином, результати детекції YOLO наповнюються розумінням контексту.

### Етап 5: Гео-агрегація (Geo Aggregation)

На цьому етапі дані з розрізнених маленьких картинок збираються в єдину картину світу. Модуль агрегації приймає об'єкти з локальними координатами (пікселі всередині тайлу 1024x1024) і переводить їх у глобальні географічні координати (GeoJSON полігони), використовуючи метадані з першого етапу.

Оскільки тайли перекриваються, один і той самий будинок міг потрапити на два сусідні знімки. Агрегатор використовує алгоритм NMS (Non-Maximum Suppression): він дивиться на географічні полігони, і якщо два об'єкти занадто сильно перетинаються (мають великий IoU), він залишає тільки той, у якого вища впевненість детекції, а дублікат видаляє.

Задача NMS та IoU прибрати дублікати, які неминуче виникають, коли ми ріжемо велике зображення на шматки з перекриттям (overlap). Один будинок може потрапити на два сусідні тайли, і детектор знайде його двічі. Нам треба залишити лише один запис.

###### IoU (Intersection over Union) або  Коефіцієнт перекриття -  математична формула, яка показує, наскільки сильно два прямокутники (або полігони) накладаються один на одного.
Якщо простіше, то це площа спільної частини поділити на загальну площу обох фігур. Значення від 0 до 1.
**0:** Фігури взагалі не торкаються. **0.5:** Перекриваються наполовину. **1.0:** Ідеально збігаються (одна фігура накладена на іншу).

###### NMS (Non-Maximum Suppression) або відсіювання немаксимумів - алгоритм, який використовує IoU, щоб залишити тільки "найкращий" прямокутник для кожного об'єкта.
Якщо  детектор знайшов 3 рамки навколо однієї вантажівки, рамка А (впевненість 0.95), рамка Б (впевненість 0.80) та рамка В (впевненість 0.75), то алгоритм NMS спочатку  всі рамки за впевненістю (score) від найбільшої до найменшої. Далі бере першу рамку (А) з найвищим балом, рахує IoU рамки А з усіма іншими (Б і В). Якщо IoU(А, Б) > 0.6 (поріг з налаштувань) — значить, Б це дублікат А. Видаляємо Б. Якщо IoU(А, В) > 0.6 — значить, В це дублікат А. Видаляємо В. Якби залишились рамки, які не перетинаються з А (наприклад, інша вантажівка поруч), ми б взяли наступну за силою рамку і повторили процес для неї.
### Етап 6: Виявлення змін (Change Detection) — опціонально

Якщо в систему завантажено два знімки однієї місцевості за різні дати (А і Б), запускається модуль змін. Спочатку він робить ко-реєстрацію: намагається ідеально накласти знімок Б на А, вираховуючи зсув у пікселях, який міг виникнути через похибки супутника.

Потім він порівнює списки гео-об'єктів. Він шукає пари об'єктів, які географічно знаходяться поруч. Якщо об'єкт був на знімку А, але зник на Б — це "removed". Якщо з'явився новий — "added". Якщо об'єкт на місці, але змінилися його атрибути (інший колір даху, змінилася форма, змінилася кількість машин поруч) — це "modified". Вихід цього етапу — звіт про зміни у форматі JSON.

### Етап 7: Індексація та Пошук (Semantic Indexing)

Фінальний етап перетворює всі зібрані дані на формат, придатний для швидкого пошуку. Текстові описи об'єктів (наприклад, "червоний цегляний склад з вантажівками поруч") перетворюються на вектори чисел (embeddings) за допомогою моделі OpenAI Embedding. Ці вектори зберігаються у векторній базі даних (FAISS). **FAISS (Facebook AI Similarity Search)** — це бібліотека, розроблена Facebook (Meta), для швидкого пошуку схожих векторів. FAISS використовує хитрі математичні трюки та структурування пам'яті, щоб знайти найближчий вектор за мілісекунди.

Такий пошук дозволяє  шукати об'єкти не за точним збігом слів (як Ctrl+F), а за **сенсом**. Наприклад, якщо користувач написав "пошкоджена техніка", система знайде об'єкти, які VLM описала як "розбита вантажівка" або "танк без башту", навіть якщо слово "техніка" там не зустрічається.

VLM генерує опис для кожного знайденого об'єкта. Наприклад: _"Сірий ангар з металевим дахом, поруч стоять три вантажівки"_. Цей текст відправляється в ембедінг модель. Вона перетворює текст на **вектор**. Схожі за змістом тексти матимуть математично близькі вектори. Всі ці вектори складаються у спеціальну структуру даних (індекс), яку створює бібліотека FAISS. Коли користувач пише запит _"промислова зона"_, він теж перетворюється на вектор. FAISS миттєво знаходить у базі вектори, які знаходяться "найближче" до вектора твого запиту.

**FAISS vs Milvus/Qdrant/Weaviate:** 

**Milvus** — це повноцінна **база даних** (сервер). Щоб її запустити, треба підіймати окремий Docker-контейнер, налаштовувати мережу, порти. **FAISS** - це **бібліотека** (просто `pip install faiss`). Вона працює всередині Python-скрипта і зберігає індекс у звичайному файлі на диску. Для локального CLI-застосунку, який обробляє один великий знімок, FAISS легший, простіший і не вимагає інфраструктури.


### Детальний опис модулів та функцій

#### Модуль `tiling.py` (Нарізка зображень)

Клас **ImageTiler**

Цей клас відповідає за перетворення великого файлу на серію координатних вікон.Функція **iter_tiles** — це генератор, який прораховує сітку координат. Вона проходить циклами по ширині та висоті зображення, створюючи об'єкти TileRef, які містять межі вікна, трансформаційну матрицю для гео-прив'язки та метадані, але не завантажують пікселі в пам'ять. Функція **read_tile_rgb** виконує фізичне зчитування даних з диска. Вона приймає конкретний TileRef, відкриває GeoTIFF файл, вирізає потрібний шматок, конвертує його в масив uint8 (стандартний формат зображення) і повертає готовий для обробки масив пікселів.

#### Модуль `gates.py` (Фільтри тайлів)

Клас **HeuristicGate**. Реалізує швидку математичну фільтрацію без нейромереж. Функція **keep** обчислює ентропію (міру хаосу в гістограмі яскравості) та щільність країв (через алгоритм **Canny**). Потім вона комбінує ці показники в єдиний бал і порівнює з пороговим значенням: якщо бал нижчий, тайл вважається порожнім.

Клас **CLIPGate**. Використовує мультимодальну модель **CLIP** для семантичної оцінки. Функція **lazy** завантажує важку модель у пам'ять лише при першому зверненні. Вона також попередньо кодує текстові промпти (наприклад, "ліс", "місто") у вектори, щоб не робити це для кожного тайлу. Функція **keep** перетворює зображення тайлу на вектор і знаходить його косинусну схожість із векторами "хороших" та "поганих" текстових описів. Якщо схожість із "хорошими" вища на заданий поріг, тайл пропускається.

Клас **CNNGate**. Використовує легку згорткову мережу (MobileNet) для бінарної класифікації. Функція **keep** змінює розмір тайлу до 224x224, нормалізує його і проганяє через нейромережу. На виході отримується одне число (ймовірність), яке порівнюється з порогом для прийняття рішення.

#### Модуль `detector.py` (Пошук об'єктів)

Клас **YOLODetector**. Обгортка над бібліотекою Ultralytics YOLO для зручного використання в пайплайні. Функція **detect** запускає інференс (передбачення) моделі на переданому зображенні. Вона розбирає сирий результат моделі, фільтрує знахідки за порогом впевненості та формує список об'єктів **DetBox** із координатами у пікселях та назвами класів.

#### Модуль `vlm.py` (Візуальна мовна модель)

Клас **VLMAnnotator**. Клієнт для взаємодії з API великих мовних моделей. Функція **annotate** це асинхронна функція, яка кодує зображення в Base64, формує довгий текстовий промпт із списком знайдених YOLO об'єктів і відправляє це на сервер. Вона також обробляє помилки мережі та повторює запити при збоях. Функція **prompt** динамічно створює текст інструкції для моделі. Вона вставляє в шаблон JSON-схему, яку модель має заповнити, та список детектованих об'єктів, щоб модель знала, куди саме дивитися.

#### Модуль `geo.py` (Геометрія та агрегація)

Клас **GeoAggregator**. Відповідає за злиття даних та роботу з координатами. Функція **build_geo_objects** перетворює піксельні координати рамок (наприклад, x=100, y=200 на тайлі) в реальні географічні координати (полігони), використовуючи **афінне перетворення** з файлу. Вона також об'єднує результати YOLO та VLM в єдиний об'єкт. Функція **geo_nms** (Non-Maximum Suppression) вирішує конфлікти дублікатів. Вона перебирає всі знайдені полігони, знаходить ті, що перетинаються більше ніж на заданий відсоток (**IoU**), і залишає серед них лише найкращий, видаляючи зайві копії, що виникли на стиках тайлів.

#### Модуль `change.py` (Виявлення змін)

Клас **Coregistrator**. Відповідає за вирівнювання двох зображень перед порівнянням. Функція **estimate_shift** використовує фазову кореляцію (**Phase Correlation**) у частотній області для знаходження зсуву між двома зображеннями. Вона повертає вектор зсуву (dx, dy), який треба застосувати до другого зображення, щоб воно ідеально лягло на перше.

Клас **ChangeDetector**. Логіка порівняння об'єктів між часом А та часом Б. Функція **diff** виконує основну роботу: зсуває об'єкти з кадру Б згідно з розрахованим зсувом, а потім шукає пари об'єктів між кадрами, використовуючи просторовий індекс (**R-tree**). Функція **modified** визначає, чи змінився об'єкт, який залишився на місці. Вона перевіряє зміни в площі, орієнтації, візуальних атрибутах (колір, матеріал) та контексті (кількість машин поруч), щоб вирішити, чи ставити позначку "modified".

#### Модуль `semantic_index.py` (Пошуковий індекс)

Клас **EmbeddingClient**. Клієнт для перетворення тексту в вектори. Функція **embed** приймає список рядків тексту і відправляє їх на API. Повертає масив нормалізованих векторів, готових до математичного порівняння.

Клас **VectorIndex** та **SemanticSearcher**. Керують збереженням та пошуком векторів. Функція **build_semantic_index** проходить по всіх знайдених об'єктах, генерує для них текстовий опис (на основі атрибутів VLM), перетворює цей опис у вектор і зберігає у файл. Функція **query** у класі SemanticSearcher перетворює запит користувача на вектор і шукає найближчі до нього вектори у базі. Вона також може фільтрувати результати за метаданими (наприклад, шукати тільки "червоні дахи").

## Параметри 
### 1. Tiling (Нарізка зображення)

Цей етап відповідає за перетворення гігантського супутникового знімка на маленькі шматочки, які здатен "переварити" комп'ютер.

**`tile_size`** - визначає ширину і висоту (у пікселях) квадратного шматка (тайла), який вирізається з великого зображення. Великі моделі (VLM/YOLO) не можуть прийняти зображення 20000x20000 пікселів. Ми мусимо їх різати.
Велике значення (2048): Менше тайлів, швидше проходить етап нарізки, але потребує більше відеопам'яті (VRAM) і VLM може "загубити" дрібні об'єкти. Мале значення (512): Краще видно дрібні деталі, але тайлів стає дуже багато і процес аналізу затягується. **Формат:** Ціле число (int), зазвичай 512, 1024, 2048. **Функції:** `ImageTiler.__init__`, `ImageTiler.iter_tiles`.

**`overlap`** - визначає зону перекриття сусідніх тайлів. Наприклад, якщо `tile_size=1024` а `overlap=256`, то наступний тайл почнеться не з 1025-го пікселя, а з 768-го.  Якщо будинок потрапить рівно на лінію розрізу без `overlap`, детектор його не побачить або побачить як два шматки. Перекриття гарантує, що об'єкт потрапить цілком хоча б на один тайл.**Формат:** Ціле число (int). Рекомендовано 10-25% від розміру тайла. **Функції:** `ImageTiler.iter_tiles`

**`bands`** - вказує, які канали з супутникового знімка брати для створення кольорової картинки. GeoTIFF часто має багато спектральних каналів (інфрачервоний, тепловий тощо). Моделі YOLO/VLM навчалися на звичайних фото (RGB). Тому нам треба вибрати саме канали Червоний, Зелений, Синій. **Формат:** Рядок з числами через кому (List[int]). **Функції:** `ImageTiler.read_tile_rgb`.

### 2. Gate (Фільтрація тайлів)

Цей етап економить гроші та час, відсіюючи пусті картинки.

**`GATE_MODE`** - Вибирає алгоритм фільтрації. `heuristic`: Швидка математика (контури + ентропія).`clip`: Семантичний аналіз (порівняння з текстом "ліс" vs "місто").`cnn`: Легка нейромережа-класифікатор. `none`: Обробляти все підряд (дуже довго). **Функції:** `_build_gate` в `cli.py`, класи в `gates.py`.

**`H_SCORE`, `H_EDGE`, `H_ENT`** (Тільки для `heuristic`) - порогові значення для математичного фільтра. `H_EDGE`: Чи багато різких ліній на фото? (Ліс — мало прямих ліній, місто — багато). `H_ENT`: Наскільки різноманітні кольори? (Море — однорідне, місто — різнокольорове). `H_SCORE`: Комбінована оцінка `0.6*Edge + 0.4*Entropy`. Якщо ви хочете знаходити навіть одинокі хатинки в лісі — зменшуйте ці параметри. Якщо тільки густу забудову — збільшуйте. **Формат:** Дробові числа (float).**Функції:** `HeuristicGate.keep`.

### 3. Detector (YOLO)

Пошук об'єктів (геометрія).

**`model_path`** вказує шлях до файлу ваг нейромережі (наприклад, `yolo12n.pt`). Вибір балансу швидкість/точність. Моделі з індексом `n` (nano) — дуже швидкі, але менш точні. Моделі `x` (extra large) — дуже точні, але повільні.
**Функції:** `YOLODetector._lazy` (завантаження моделі).

**`conf`** - Поріг впевненості (Confidence Threshold). Модель для кожної знахідки видає ймовірність (наприклад, "це машина на 40%"). Якщо `conf=0.5`, система ігнорує всі знахідки з впевненістю нижче 50%. Низький поріг (0.1):  Багато сміття, але не пропустиш об'єкти. Високий поріг (0.8): Тільки 100% об'єкти, але багато пропусків. **Функції:** `YOLODetector.detect`.

**`max_det`** - максимальна кількість об'єктів, яку дозволено знайти на **одному** тайлі. Захист від зависання, якщо модель "збожеволіє" і почне бачити тисячі об'єктів у шумі. **Функції:** `YOLODetector.detect`.

**`NMS IOU`** - поріг перекриття для видалення дублікатів. Якщо один будинок знайдено двічі (через `overlap`), NMS дивиться: якщо рамки перекриваються більше ніж на `NMS IOU` (наприклад, 60%), то слабша рамка видаляється. Високе значення (0.9): Видаляє тільки майже повні копії. Може залишити дублі. Низьке значення (0.3): Агресивно видаляє все, що поруч. Може видалити машини на щільній парковці. **Функції:** `GeoAggregator.geo_nms`.
### 4. VLM (Візуальна мовна модель)

Опис об'єктів (семантика).

**`base_url`, `api_key`, `model`**- стандартні параметри підключення до API (OpenAI, локальний vLLM тощо). **Функції:** `VLMAnnotator.__init__`, `VLMAnnotator.annotate`.

**`concurrency`** - кількість одночасних запитів до VLM. Для OpenAI: Можна ставити 5-10, щоб прискорити роботу. Для локальної моделі (vllm): Краще ставити 1-2, бо відеокарта не витримає багато паралельних обробок зображень. **Функції:** `VLMAnnotator` (використовує `asyncio.Semaphore`).

**`VLM_IMAGE_MODE`** - визначає, як передати картинку на сервер. `base64`: Картинка кодується в текст і вставляється прямо в запит (надійніше для локальних мереж). `url`: Передається посилання (якщо картинка доступна з інтернету). **Функції:** `VLMAnnotator.annotate` (виклик `_to_png_b64`).

**`VLM_MAX_IMAGE_SIDE`** - зменшує розмір картинки перед відправкою до VLM. Деякі моделі (як Qwen-VL) погано працюють або дуже повільні з картинками 1024x1024. Зменшення до 768px прискорює генерацію без сильної втрати якості. **Функції:** (Внутрішня логіка `VLMAnnotator`, якщо реалізовано).

**`allow_empty_vlm` (прапорець "Run VLM even without detections")** - примусово відправляє тайл у VLM, навіть якщо YOLO нічого не знайшов. Якщо ми хочемо отримати загальний опис місцевості ("сцена: густий ліс"), навіть якщо там немає машин чи будинків. **Функції:** `process_image` (у `pipeline.py`).

### 5. Output & Execution (Виконання)

**`max_inflight`** - контролює розмір черги обробки. Це скільки тайлів одночасно знаходяться в пам'яті програми (читаються, детектуються або відправляються в API). Контроль оперативної пам'яті (RAM). Якщо поставити занадто багато, програма "впаде". Оптимально 16-32. **Функції:** `process_image` (semaphore).

**`cache_dir`** - шлях до папки, де зберігаються JSON-відповіді VLM. Якщо ви перезапускаєте аналіз з іншими параметрами (наприклад, змінили NMS), система не буде повторно платити гроші за VLM-аналіз тих самих тайлів, а візьме результат з диска. **Функції:** `TileCache.get`, `TileCache.put`.
### 6. Index (Пошук)

**`rail_geojson`** - шлях до файлу з лініями залізниць. Додає до кожного об'єкта метадані "відстань до колії". Це дозволяє робити запити типу "склади не далі 500м від залізниці". **Функції:** `build_semantic_index`, `compute_rail_dist_m`.

**`INDEX_TILES`** - чи додавати в пошукову базу самі тайли як окремі документи. Зазвичай ми шукаємо _об'єкти_ (машини, будинки). Але якщо цей прапорець увімкнено, можна шукати _місцевість_ (наприклад, "знайди тайли, де є густий дим" або "тайли з болотом"), навіть якщо окремі об'єкти там не виділені. **Функції:** `build_semantic_index` (блок `if index_tiles...`).



### Опис інтерфейсу вкладки **Analyze** (Аналіз).

Це "пульт керування" системою, де ви обираєте зображення, запускаєте процес обробки та спостерігаєте за його ходом у реальному часі.

**Runs directory** 

Це шлях до папки, де будуть зберігатися результати аналізу. Для кожного натискання кнопки "Run analysis" система створює тут нову підпапку з унікальним ім'ям (наприклад, 20251111_0948_image_name). У цій папці будуть лежати всі вихідні файли: нарізані картинки (якщо увімкнено), JSON-файли з об'єктами та логи.
Це потрібно для того, щоб ви могли повернутися до результатів старого аналізу, не запускаючи його заново.

**Input directory (GeoTIFF/JP2/PNG)**

Це шлях до папки на вашому комп'ютері або сервері, де лежать вихідні супутникові знімки. Система автоматично сканує цю папку і показує всі знайдені файли підтримуваних форматів у випадному списку нижче.

**Satellite image (Випадний список)**

Тут ви обираєте конкретний файл для обробки. Рядок під списком (наприклад, 684636885...tif | 2212.8 MB | 2025-11-11...) — це інформаційна картка вибраного файлу.

- **Ім'я файлу:** Довга назва вашого GeoTIFF.
- **Розмір (2212.8 MB):** Важливий показник. Файл на 2 ГБ буде оброблятися значно довше, ніж файл на 100 МБ. Це допомагає оцінити час очікування.
- **Дата:** Час останньої зміни файлу.
### 2. Кнопки керування (Action Buttons)

Ці кнопки запускають складні процеси "під капотом".

**Run analysis (Запустити аналіз)**

Це головна кнопка "Старт". Коли ви її натискаєте, система запускає фоновий процес (Job), який виконує весь пайплайн:

1. Нарізає обраний GeoTIFF на тайли.
2. Фільтрує їх через Gate.
3. Шукає об'єкти через YOLO.
4. Робить запити до VLM для опису.
5. Збирає все в єдиний файл geoobjects.jsonl.

Ця кнопка блокує можливість зміни параметрів, доки процес не завершиться або не впаде з помилкою.

**Build / rebuild index (Побудувати індекс)**

Цю кнопку натискають після завершення аналізу. Вона бере готовий файл з об'єктами (geoobjects.jsonl) і перетворює текстові описи на вектори для пошуку (використовуючи FAISS). Побудова індексу вимагає часу, тому система не робить це автоматично, а чекає вашого рішення. Без натискання цієї кнопки вкладка **"Search"** не знайде нових об'єктів.

**Refresh status (Оновити статус)**

Інтерфейс (Streamlit) іноді не оновлюється автоматично миттєво. Ця кнопка примусово зчитує стан фонового процесу. Якщо вам здається, що прогрес "завис", натисніть її, щоб отримати актуальні цифри.

**Open last results (Відкрити останні результати)**

Ця кнопка виводить на екран шлях до папки останнього запуску. Це зручно, якщо ви хочете відкрити цю папку в провіднику Windows/Finder, щоб вручну переглянути файли або забрати звіт.

### Метрики та Статус (Dashboard)

Ця панель показує, що відбувається прямо зараз.

**Status: ok**

Поточний стан процесу.

- `running`: Процес іде.
- `ok` / `done`: Аналіз успішно завершено.
- `error`: Сталася помилка (деталі будуть у логах нижче).

**tiles (673/1640)**

Прогрес-бар обробки.

- **1640 (інше число):** Загальна кількість шматків, на які було розрізано велике зображення. Це число залежить від `tile_size` та розміру картинки.
- **673 (перше число):** Скільки шматків система вже встигла пройти (перевірити фільтром, і якщо треба — детектором та VLM).

**kept (342)**

Кількість тайлів, які пройшли фільтрацію (Gate). У вашому прикладі з 673 оброблених тайлів лише 342 виявилися "цікавими" (будівлі, дороги), а 331 тайл був відсіяний як "сміття" (ліс/вода). Це показує ефективність фільтра.

**dets (180)**

**"Raw detections"** — скільки всього рамок знайшла модель YOLO на всіх збережених тайлах до чистки дублікатів. Це число швидко зростає в процесі роботи.

**objects (0)**

**"Final objects"** — кількість унікальних гео-об'єктів після фінальної агрегації. Це число часто залишається `0` до самого кінця роботи. Агрегація (видалення дублікатів NMS та склеювання координат) — це останній крок, який виконується, коли всі тайли вже оброблені. Тому, якщо процес ще йде (673/1640), нуль тут — це нормально.

**cache hit % (100.0)**

Показник економії. 100.0% означає, що для всіх 342 "цікавих" тайлів система **не робила** запитів до API VLM. Вона знайшла готові описи в папці `cache/` (від попереднього запуску). Це означає, що цей запуск був дуже швидким. Якщо тут 0%, значить, система активно витрачає час на API.

**Logs (last 200 lines)**

Текстове вікно внизу. Воно транслює технічні повідомлення: Який саме тайл зараз обробляється (наприклад, `tile_id: r001_c005`). Чому тайл був відсіяний (наприклад, `Gate filtered: score 0.1 < 0.24`). Помилки підключення до мережі або API. Це перше місце, куди треба дивитися, якщо Status змінився на error.

### Опис функціонала вкладок Search та Compare
_пояснює призначення кожного елемента інтерфейсу та логіку їхньої роботи без використання списків._

**Вкладка Search (Пошук)**

Ця вкладка відповідає за семантичний пошук по базі вже проаналізованих об'єктів. Робота починається з поля **Runs directory**, яке вказує шлях до кореневої папки з результатами попередніх запусків. Система сканує цю директорію на наявність папок з індексами, щоб зрозуміти, де саме шукати дані. Для перетворення текстового запиту користувача на числовий вектор, зрозумілий комп'ютеру, використовуються параметри підключення до моделі ембеддингів. Поля **Embeddings base_url**, **Embeddings api_key** та **Embeddings model** задають адресу API, ключ доступу та назву моделі, яка виконує цю векторизацію. Критично важливо, щоб ці параметри збігалися з тими, що використовувалися **при побудові індексу**, інакше вектори запиту та бази будуть несумісними.

Основна взаємодія відбувається через поле **Запит**, куди ви вводите опис шуканого об'єкта природною мовою, наприклад "зруйнований склад" або "червоний дах". Повзунок **top_k** регулює кількість результатів, які поверне система. Значення варіюється від 1 до 100, де менші числа дають лише найбільш релевантні збіги, а більші дозволяють побачити ширший спектр, але з меншою точністю.

Для уточнення результатів використовується група фільтрів. Параметр **Scope** обмежує область пошуку: ви можете шукати серед усіх записів **(all**), конкретних об'єктів **(object)**, цілих тайлів **(tile)** або зафіксованих змін **(change)**. Поля **label**, **roof_color** та **change_type** працюють як жорсткі фільтри за метаданими. Якщо ви оберете конкретний колір даху або тип об'єкта, система відкине всі результати, що не відповідають цьому критерію, навіть якщо вони семантично схожі на текстовий запит. Параметр **rail_dist_max_m** є просторовим фільтром, який дозволяє знаходити об'єкти лише в межах заданої відстані в метрах від залізничних колій, якщо відповідний шар геоданих був завантажений при індексації.

**Вкладка Compare (Порівняння)**

Цей розділ призначений для виявлення змін (**Change Detection**) між двома різними знімками однієї й тієї ж місцевості. Робота починається з поля **Runs directory**, яке, аналогічно до пошуку, вказує системі, де лежать папки з готовими аналізами. На основі цього шляху заповнюються випадні списки **Run A** та **Run B**. У цих полях ви обираєте два конкретні запуски, які будуть порівнюватися: **Run A** зазвичай виступає як базовий (старіший) знімок, а **Run B** як новий, на якому шукаються відмінності.

Параметр **MATCH_IOU (Intersection over Union)** визначає поріг геометричної схожості, необхідний для того, щоб система вважала два об'єкти з різних дат одним і тим самим. Якщо ступінь перекриття об'єктів менший за це значення, система вирішить, що старий об'єкт зник, а на його місці з'явився новий, або що вони взагалі не пов'язані.

Параметр **BUFFER_TOL (m)** задає допуск у метрах для компенсації похибок геоприв'язки супутникових знімків. Оскільки ідеального накладання досягти важко, цей буфер дозволяє "знайти" відповідний об'єкт на другому знімку, навіть якщо його координати трохи змістилися через технічні причини. Параметр **VEH_R (m)** специфічний для аналізу транспорту і задає радіус кластеризації транспортних засобів. Замість того, щоб порівнювати кожну машину окремо (що складно через їх рухливість), система групує техніку в радіусі заданих метрів і порівнює кількість об'єктів у групі, що дозволяє фіксувати зміни щільності транспорту на парковках чи стоянках.

## Інструкція для аналітиків: як працювати з папкою тайлу

Папка тайлу — це **повний аналітичний пакет** для одного фрагмента супутникового зображення.  
Вона створена так, щоб ви могли оцінити ситуацію **без доступу до всієї системи**, маючи лише ці файли.

Ваша базова логіка роботи завжди така:  
спочатку ви дивитесь **що реально видно на зображенні**, потім — **що побачили алгоритми**, і лише після цього — **як це було інтерпретовано та прив’язано до місцевості**.

Жоден файл у цій папці не є «істиною сам по собі». Кожен з них — це **рівень доказу**, який потрібно звіряти з іншими.

**tile.png** — це саме зображення тайлу, з яким працювали всі алгоритми. Для вас це **першоджерело**, і будь-який висновок у JSON-файлах має підтверджуватись тим, що ви бачите тут. Якщо на tile.png ви **не бачите** того, що написано в описах, — це сигнал до сумніву, а не до довіри системі.

**dets.json** — що знайшов детектор. Цей файл показує, **де саме алгоритм виявив потенційні об’єкти**.  
Тут немає “людського сенсу” — лише прямокутники, впевненість і технічні класи. Коли ви читаєте dets.json, памʼятайте:   низький **score** означає слабкий сигнал, але **не означає, що обʼєкта немає**;   **cls** — це припущення моделі, а не підтверджений тип обʼєкта. Використовуйте **dets.json**, щоб зрозуміти, **на що звернула увагу система**, але не щоб робити висновки.

**semantics.json** — словесний опис сцени.Цей файл містить **людськоподібний опис того, що видно на тайлі**, згенерований VLM. Саме його ви будете читати найчастіше, коли переглядаєте багато тайлів. Сприймайте **semantics.json** як **аналітичну замітку**, а не як факт.   Він корисний для швидкого орієнтування, пошуку і фільтрації, але всі твердження з нього потрібно звіряти з **tile.png**. Особливу увагу звертайте на поле **notes**:  
якщо там є невпевненість або застереження — це означає, що навіть модель “не впевнена в тому, що бачить”.

**geoobjects.json** — обʼєкти на мапі. Цей файл відповідає на питання: **де саме на місцевості знаходиться те, що ми бачимо**.   Усе, що тут описано, вже можна відобразити в GIS, накласти на інші шари або використовувати для просторового пошуку. Для аналітика **geoobjects.json** важливий тим, що він дозволяє:  
– перевірити близькість обʼєкта до доріг, населених пунктів, інфраструктури;  
– співставити знахідки з іншими джерелами OSINT;  
– побачити, чи повторюються подібні обʼєкти на суміжних тайлах.

**Label** і **attributes** тут — це **інтерпретація**, а geometry — **факт місцезнаходження**.

**tile.json** — паспорт і аудит. Це головний файл, якщо вам потрібно зрозуміти **чому цей тайл взагалі тут зʼявився**.  Він містить походження тайлу, параметри вирізання, логіку відбору та всі проміжні результати.
Коли виникають питання типу:  
– чому цей тайл був проаналізований, а сусідній — ні;  
– чому модель дала саме такий результат;  
– чи можна відтворити цей аналіз пізніше;
відповіді майже завжди знаходяться саме в tile.json.

Звертайте особливу увагу на **gate_ok** і **gate_metrics**:  
вони пояснюють, **чому система вважала цей тайл “цікавим”**, а не порожнім.

####  Детальне пояснення dets.json

### det_id
Це **локальний ідентифікатор об’єкта в межах конкретного тайлу**.  
Він існує для того, щоб всі наступні файли могли говорити про один і той самий об’єкт без плутанини.

### bbox_px
Це координати прямокутника у пікселях на tile.png.  
Користувачу це потрібно, щоб точно знати, **яку частину зображення мав на увазі алгоритм**, і перевірити це візуально.

### score
Це впевненість YOLO в тому, що “щось тут є”.  
Аналітик має знати цей параметр, щоб розуміти, чи це сильна знахідка, чи слабкий сигнал, який потребує обережності.

### cls
Це назва класу, який повернула модель детекції.  
Важливо знати, що це **не істина**, а лише підказка, і саме тому вона не використовується напряму як фінальний лейбл.

#### Детальне пояснення geoobjects.json

### image_id
Це ідентифікатор оригінального супутникового знімка.  
Він дозволяє відстежити походження об’єкта і повернутися до джерела при необхідності перевірки.

### obj_id
Це **глобальний унікальний ідентифікатор об’єкта**.  
Саме його використовують пошук, мапа і аналітичні звіти, а не det_id.

### label
Це **нормалізований опис об’єкта**, який уже має сенс для людини.  
Цей label — те, що аналітик бачить у фільтрах і легенді мапи.

### confidence
Це числова оцінка надійності об’єкта.  
Вона дозволяє сортувати результати і швидко відсіювати шум.

### geometry
Це геометрія об’єкта у координатах місцевості, а не в пікселях.  
Без цього файлу система бачила б “щось на картинці”, але не знала б, де це на Землі.

#### geometry.type
Вказує, як саме потрібно інтерпретувати координати.  
Це критично для правильного рендеру і просторових обчислень.

#### geometry.coordinates
Це фактичні вершини полігону на місцевості.  
Саме ці координати використовуються для перетинів, буферів і пошуку поблизу.

### attributes
Це уточнюючі характеристики об’єкта.  
Вони дозволяють шукати не просто “об’єкти”, а, наприклад, транспорт певного типу.

### tile_id
Це зворотне посилання на тайл.  
Воно дозволяє аналітику відкрити повний контекст, а не дивитися на об’єкт у вакуумі.

#### Детальне пояснення semantics.json

### tile_id
Зв’язує семантичний опис з конкретним тайлом.  
Це гарантія, що текстовий опис не “відірветься” від зображення.

### scene
Це короткий опис загальної сцени тайлу.  
Він дозволяє швидко зрозуміти контекст без читання всіх деталей.

### annotations
Це список описів об’єктів на тайлі.  
Він поєднує детекцію, опис і атрибути в одному місці.

#### det_id
Дозволяє точно знати, **який bbox описується**.  
Без цього опис був би абстрактним і нефальсифікованим.

#### label
Це словесне визначення того, як об’єкт виглядає.  
Саме тут відбувається перехід від “алгоритм побачив” до “людина розуміє”.

#### attributes
Це деталі, які неможливо передати одним словом.  
Вони критичні для глибокого пошуку і аналітики.

#### notes
Це простір для невпевненості або додаткових пояснень.  
Аналітику важливо бачити не тільки результат, а й межі впевненості системи.


#### Детальне пояснення tile.json

### image_id
Дозволяє згрупувати тайли одного знімка.  
Це важливо для аналізу великих територій.

### tile_id
Головний ідентифікатор тайлу.  
Він є ключем для всієї структури файлів.

### row / col
Описують положення тайлу в сітці.  
Завдяки цьому можна відновити просторове сусідство тайлів.

### window
Описує, яку саме частину GeoTIFF було вирізано.  
Це необхідно для точного відтворення тайлу з оригіналу.

### bounds
Географічні межі тайлу.  
Вони дозволяють миттєво зрозуміти, де цей тайл знаходиться на мапі.

### transform
Описує математичний зв’язок між пікселями і координатами.  
Без нього неможливо коректно перевести bbox у геометрію.

### crs_wkt
Описує систему координат.  
Аналітику це важливо, щоб знати, у якій проєкції виконані виміри.

### gate_ok
Пояснює, **чому цей тайл взагалі аналізувався**.  
Це ключ до розуміння логіки відбору.

### gate_metrics
Дає кількісне пояснення рішення gate.  
Це критично для налаштування і довіри до системи.

### class_counts
Швидкий огляд того, що побачив детектор.  
Аналітик може одразу оцінити, чи варто дивитися тайл.

### dets
Повний список детекцій.  
Це зв’язок між сирими даними і всіма наступними рівнями.

### semantics
Вбудований семантичний результат для зручності доступу.  
Це дозволяє не відкривати окремий файл під час перегляду.

### tile_hash
Захищає від повторної обробки і дублювання.  
Аналітику це важливо для довіри до кешу.

### prompt_id / vlm_prompt_id
Дають відповідь на питання “чому модель сказала саме так”.  
Це критично для порівняння результатів у часі.

### detector_model / vlm_model
Дають повну відтворюваність результатів.  
Без цього будь-який аналіз втрачає наукову і OSINT-цінність.

